---
layout: post
title: "学习论文中的数学"
date:  2026-01-28 01:11:53 +0800
mathjax: true
mermaid: false
categories: Math
---

和豆包讨论了一下数学学习相关的问题，这里记录总结一下。

Prompt:

> 我想聊聊论文中的数学。从去年 9 月份以来，我开始从事 AI 相关的工作，也看了不少论文。现在看论文，理解论文的思路和算法过程中，不可避免的涉及到数学。遗憾的是，我只有本科文凭的数学能力，而且很多都忘记了，所以看论文中的数学公式感觉非常吃力。现在，我意识到我需要把数学能力逐步拾起来，努力从一个单纯的软件工程师向算法工程师或者研究员的领域前进，成为一个既懂工程又懂算法的人才。请你给我一些建议，我如何把论文中的数学看懂？尤其是大量的希腊符号，各种 Notation

我整理的回答：

首先，让我们先有个查表的东西，这个是基础：

# AI论文高频希腊字母&数学符号速查表
> 适用场景：深度学习/大模型/机器学习顶会论文（NeurIPS/ICML/ICLR/CVPR/EMNLP等），配套PyTorch工程实现，解决符号看不懂、notation不熟悉的问题。
> 使用方式：遇到陌生符号直接检索，结合代码对应关系记忆，边读论文边补充个性化条目。

## 一、高频希腊字母核心速查表
| 希腊字母符号 | 标准读音 | 论文常用指代 | 核心数学/工程含义 | 对应PyTorch代码场景 |
| :--- | :--- | :--- | :--- | :--- |
| $\alpha$ | 阿尔法 | $\alpha$ | 学习率、初始化系数、随机采样系数 | 优化器学习率 `lr`、随机种子相关系数、初始化超参 |
| $\beta$ | 贝塔 | $\beta$ | 优化器动量系数、二阶矩估计系数 | Adam优化器 `betas=(beta1, beta2)`、动量SGD的动量项 |
| $\gamma$ | 伽马 | $\gamma$ | 缩放系数、激活函数缩放参数、权重系数 | LayerNorm/BatchNorm的缩放参数、LeakyReLU斜率 |
| $\delta$ / $\Delta$ | 德尔塔 | $\Delta$ | 增量、差值、梯度更新步长 | 参数更新的变化量、损失函数差值、梯度裁剪阈值 |
| $\epsilon$ | 艾普西隆 | $\epsilon$ | 极小常量、数值平滑项、噪声扰动 | 防止除0的 `eps=1e-8`、LabelSmoothing、高斯噪声 |
| $\theta$ | 西塔 | $\theta$ | 模型全部可训练参数集合 | 模型所有权重`weight`、偏置`bias`的统称，$\theta=\{w_1,w_2,...,w_n\}$ |
| $\eta$ | 伊塔 | $\eta$ | 学习率（与$\alpha$通用） | 部分论文替代$\alpha$表示优化器学习率`lr` |
| $\lambda$ | 兰姆达 | $\lambda$ | 正则化系数、权重衰减系数、拉格朗日乘子 | L1/L2正则 `weight_decay`、约束优化的惩罚系数 |
| $\mu$ | 缪 | $\mu$ | 均值、期望、分布中心 | 正态分布均值、BatchNorm的均值、数据分布期望 |
| $\sigma$ | 西格玛 | $\sigma$ | 标准差、方差、分布离散程度 | 正态分布标准差、数据归一化的标准差参数 |
| $\Sigma$ | 大写西格玛 | $\sum$ | 累加求和算子 | 损失函数样本累加、注意力加权求和、批量损失求和 |
| $\Pi$ | 大写派 | $\prod$ | 累乘算子 | 概率模型的似然估计、联合概率计算 |
| $\nabla$ | 纳布拉 | $\nabla$ | 梯度算子 | $\nabla_\theta \mathcal{L}$ 损失对参数求梯度，对应`loss.backward()` |
| $\partial$ | 偏德尔塔 | $\partial$ | 偏导数符号 | $\frac{\partial \mathcal{L}}{\partial w_i}$ 损失对单个参数的偏导 |

## 二、AI论文专用数学符号&花体符号速查
| 符号格式 | 符号名称 | 核心含义 | 工程对应 |
| :--- | :--- | :--- | :--- |
| $\mathcal{L}$ | 花体L | 损失函数、目标函数、优化目标 | 代码中定义的`loss`、总损失、分类损失、回归损失 |
| $\mathbb{E}$ | 黑板体E | 数学期望 | 概率期望、样本集上的损失期望、分布期望计算 |
| $\|\boldsymbol{x}\|$ | 范数符号 | 向量/矩阵范数（L1/L2等） | 梯度裁剪、正则化、向量相似度计算 |
| $\boldsymbol{x}$ | 粗体小写 | 向量 | 1维张量、特征向量、词嵌入向量 |
| $\boldsymbol{X}$ | 粗体大写 | 矩阵 | 2维及以上张量、权重矩阵、特征矩阵、Q/K/V矩阵 |
| $x^{(i)}$ | 上标(i) | 第i个训练样本 | 数据集的单条样本、批量数据的第i个元素 |
| $\boldsymbol{x}^T$ | 上标T | 向量/矩阵转置 | PyTorch中`tensor.T`、`torch.transpose()` |
| $x_i$ | 下标i | 向量的第i个分量 | 张量的指定维度元素、参数向量的单个参数 |

## 三、AI顶会通用Notation书写规范
读懂论文公式的前提是遵守全球机器学习领域统一的符号约定，无需死记，形成直觉即可：
1. **标量**：小写斜体普通字母，如 $x, y, w, b$，代表单个数值，对应代码中的浮点/整型变量。
2. **向量**：小写粗体字母，如 $\boldsymbol{x}, \boldsymbol{w}, \boldsymbol{v}$，代表一维张量，是标量的有序集合。
3. **矩阵**：大写粗体字母，如 $\boldsymbol{W}, \boldsymbol{X}, \boldsymbol{Q}, \boldsymbol{K}$，对应代码中2D/高维Tensor，是模型的核心参数载体。
4. **张量**：高于二维的数组，论文中常沿用矩阵粗体大写表示，工程中直接对应框架的Tensor类型。
5. **上下标规则**
    - 上标：多代表样本序号、转置、幂运算、迭代步数。
    - 下标：多代表向量分量、矩阵行列索引、特征维度。


> 接下来，如果遇到上面没有的符号或者不知道 Latex markdown 怎么写的符号，可以去 [Detexify](https://detexify.kirelabs.org/classify.html) 去搜一下。

好，在我们有了这些基础知识后，接下来就是，**每精读一篇论文，把论文中的数学符号搞懂**，并使用博客记录一下。

我理解，这个东西就像是练字，不需要所有字都练一遍，只要认真的练习几次，效果就会很好~
